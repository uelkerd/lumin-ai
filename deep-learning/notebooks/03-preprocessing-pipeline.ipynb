{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# LUMIN.AI: Text Preprocessing Pipeline for Democratic Governance Analysis\n",
    "\n",
    "This notebook demonstrates the complete text preprocessing pipeline for democratic governance text analysis,\n",
    "showcasing the functionality of our custom preprocessing modules.\n",
    "\"\"\"\n",
    "\n",
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path to import our modules\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_palette(\"deep\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "plt.rcParams[\"font.size\"] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 1. Import Custom Preprocessing Modules\n",
    "\n",
    "First, we'll import our custom preprocessing modules designed specifically for governance text analysis.\n",
    "\"\"\"\n",
    "\n",
    "# Import our custom modules\n",
    "try:\n",
    "    from src.preprocessing import (\n",
    "        text_cleaning,\n",
    "        tokenization,\n",
    "        feature_extraction,\n",
    "        data_augmentation,\n",
    "    )\n",
    "    from src.utils import config\n",
    "\n",
    "    print(\"✅ Successfully imported custom preprocessing modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importing modules: {e}\")\n",
    "    print(\n",
    "        \"Make sure you're running this notebook from the deep-learning directory or have added it to PYTHONPATH\"\n",
    "    )\n",
    "\n",
    "# Check if all required modules are available\n",
    "required_modules = {\n",
    "    \"text_cleaning\": text_cleaning,\n",
    "    \"tokenization\": tokenization,\n",
    "    \"feature_extraction\": feature_extraction,\n",
    "    \"data_augmentation\": data_augmentation,\n",
    "    \"config\": config,\n",
    "}\n",
    "\n",
    "for name, module in required_modules.items():\n",
    "    functions = [\n",
    "        func\n",
    "        for func in dir(module)\n",
    "        if callable(getattr(module, func)) and not func.startswith(\"_\")\n",
    "    ]\n",
    "    print(f\"{name}: {len(functions)} functions available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 2. Load Sample Governance Text Data\n",
    "\n",
    "For this demonstration, we'll use a sample dataset of governance-related text.\n",
    "In a real-world scenario, we would use the Austria Democracy Radar dataset.\n",
    "\"\"\"\n",
    "\n",
    "# Sample governance text data (placeholder for actual dataset)\n",
    "sample_texts = [\n",
    "    \"The democratic institutions must be strengthened to ensure proper representation of all citizens.\",\n",
    "    \"Government policies should focus on transparency and accountability to build public trust.\",\n",
    "    \"The election commission has reported significant improvement in voter participation this year.\",\n",
    "    \"Civil society organizations play a crucial role in democratic oversight and citizen engagement.\",\n",
    "    \"Parliamentary debates highlighted concerns about the separation of powers in recent legislation.\",\n",
    "    \"Freedom of the press is essential for maintaining democratic norms and government accountability.\",\n",
    "    \"Voter registration processes need to be simplified to increase electoral participation.\",\n",
    "    \"Constitutional courts provide important checks on executive power in modern democracies.\",\n",
    "    \"Public policy research indicates that democratic backsliding is occurring in several regions.\",\n",
    "    \"Political parties must engage in constructive dialogue to overcome polarization in society.\",\n",
    "]\n",
    "\n",
    "# Sample labels (for demonstration purposes)\n",
    "sample_labels = [\n",
    "    \"institutional_reform\",\n",
    "    \"governance_quality\",\n",
    "    \"electoral_process\",\n",
    "    \"civil_society\",\n",
    "    \"legislative_process\",\n",
    "    \"media_freedom\",\n",
    "    \"electoral_process\",\n",
    "    \"judicial_independence\",\n",
    "    \"democratic_backsliding\",\n",
    "    \"political_discourse\",\n",
    "]\n",
    "\n",
    "# Convert to a DataFrame for easier manipulation\n",
    "df = pd.DataFrame({\"text\": sample_texts, \"label\": sample_labels})\n",
    "\n",
    "# Display the dataset\n",
    "print(f\"Sample dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 3. Text Cleaning\n",
    "\n",
    "Apply our custom text cleaning functions to the dataset.\n",
    "\"\"\"\n",
    "\n",
    "# Apply text cleaning with default settings\n",
    "cleaned_texts = text_cleaning.clean_text_batch(df[\"text\"].tolist())\n",
    "\n",
    "# Create a comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\"original\": df[\"text\"], \"cleaned\": cleaned_texts})\n",
    "\n",
    "# Display the comparison\n",
    "comparison_df.head(3)\n",
    "\n",
    "# Examine how cleaning affects text statistics\n",
    "\n",
    "\n",
    "def text_stats(text_list):\n",
    "    \"\"\"Calculate basic text statistics\"\"\"\n",
    "    avg_len = sum(len(text) for text in text_list) / len(text_list)\n",
    "    avg_words = sum(len(text.split()) for text in text_list) / len(text_list)\n",
    "    return {\n",
    "        \"avg_length\": avg_len,\n",
    "        \"avg_words\": avg_words,\n",
    "        \"total_chars\": sum(len(text) for text in text_list),\n",
    "    }\n",
    "\n",
    "\n",
    "# Compare statistics before and after cleaning\n",
    "orig_stats = text_stats(df[\"text\"].tolist())\n",
    "clean_stats = text_stats(cleaned_texts)\n",
    "\n",
    "stats_df = pd.DataFrame({\"Original\": orig_stats, \"Cleaned\": clean_stats})\n",
    "\n",
    "print(\"Text statistics before and after cleaning:\")\n",
    "stats_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 4. Tokenization\n",
    "\n",
    "Apply our custom tokenization functions, which are specifically designed to handle governance text.\n",
    "\"\"\"\n",
    "\n",
    "# Apply standard tokenization\n",
    "standard_tokens = [tokenization.tokenize_text(text) for text in cleaned_texts[:3]]\n",
    "\n",
    "# Apply governance-specific tokenization with phrase preservation\n",
    "governance_tokens = [\n",
    "    tokenization.custom_governance_tokenizer(text) for text in cleaned_texts[:3]\n",
    "]\n",
    "\n",
    "# Display the comparison\n",
    "tokenization_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Cleaned Text\": cleaned_texts[:3],\n",
    "        \"Standard Tokens\": [\", \".join(tokens) for tokens in standard_tokens],\n",
    "        \"Governance Tokens\": [\", \".join(tokens) for tokens in governance_tokens],\n",
    "    }\n",
    ")\n",
    "\n",
    "tokenization_df\n",
    "\n",
    "# Show preserved governance phrases\n",
    "print(\"\\nPreserved governance phrases (sample):\")\n",
    "for phrase in tokenization.GOVERNANCE_PHRASES[:10]:\n",
    "    print(f\"- {phrase}\")\n",
    "\n",
    "# Compare token counts\n",
    "std_token_counts = [len(tokens) for tokens in standard_tokens]\n",
    "gov_token_counts = [len(tokens) for tokens in governance_tokens]\n",
    "\n",
    "token_counts_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Standard Tokenization\": std_token_counts,\n",
    "        \"Governance Tokenization\": gov_token_counts,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nToken counts comparison:\")\n",
    "token_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 5. Feature Extraction\n",
    "\n",
    "Transform the tokenized text into numerical features using different approaches:\n",
    "1. Bag of Words (BoW)\n",
    "2. TF-IDF\n",
    "3. Word embeddings (Word2Vec)\n",
    "\"\"\"\n",
    "\n",
    "# Prepare all tokenized texts for feature extraction\n",
    "all_governance_tokens = [\n",
    "    tokenization.custom_governance_tokenizer(text) for text in cleaned_texts\n",
    "]\n",
    "\n",
    "# Create Bag of Words features\n",
    "bow_features, bow_vectorizer = feature_extraction.create_bow_features(\n",
    "    cleaned_texts, **config.DEFAULT_FEATURE_EXTRACTION_CONFIG.get(\"bow_params\", {})\n",
    ")\n",
    "\n",
    "print(f\"Bag of Words features shape: {bow_features.shape}\")\n",
    "print(f\"Number of unique terms: {len(bow_vectorizer.get_feature_names_out())}\")\n",
    "print(\"\\nSample terms:\")\n",
    "print(bow_vectorizer.get_feature_names_out()[:10])\n",
    "\n",
    "# Create TF-IDF features\n",
    "tfidf_features, tfidf_vectorizer = feature_extraction.create_tfidf_features(\n",
    "    cleaned_texts, **config.DEFAULT_FEATURE_EXTRACTION_CONFIG.get(\"tfidf_params\", {})\n",
    ")\n",
    "\n",
    "print(f\"\\nTF-IDF features shape: {tfidf_features.shape}\")\n",
    "\n",
    "# Train a Word2Vec model on our tokenized documents\n",
    "w2v_model = feature_extraction.create_word2vec_embeddings(\n",
    "    all_governance_tokens,\n",
    "    **config.DEFAULT_FEATURE_EXTRACTION_CONFIG.get(\"word2vec_params\", {}),\n",
    ")\n",
    "\n",
    "print(f\"\\nWord2Vec model vocabulary size: {len(w2v_model.wv.index_to_key)}\")\n",
    "\n",
    "# Generate document embeddings by averaging word vectors\n",
    "word_vectors = {word: w2v_model.wv[word] for word in w2v_model.wv.index_to_key}\n",
    "doc_embeddings = feature_extraction.create_document_embeddings(\n",
    "    all_governance_tokens, word_vectors\n",
    ")\n",
    "\n",
    "print(f\"Document embeddings shape: {doc_embeddings.shape}\")\n",
    "\n",
    "# Visualize the most similar words to a governance term\n",
    "governance_term = \"democracy\"\n",
    "if governance_term in w2v_model.wv:\n",
    "    similar_words = w2v_model.wv.most_similar(governance_term, topn=10)\n",
    "    similar_df = pd.DataFrame(similar_words, columns=[\"word\", \"similarity\"])\n",
    "    print(f\"\\nWords most similar to '{governance_term}':\")\n",
    "    print(similar_df)\n",
    "else:\n",
    "    print(\n",
    "        f\"\\nThe term '{governance_term}' is not in the vocabulary (try with a larger dataset)\"\n",
    "    )\n",
    "\n",
    "# If the model has enough vocabulary, let's visualize word embeddings\n",
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Get embeddings for the most common words\n",
    "    words = w2v_model.wv.index_to_key[:30]\n",
    "    embeddings = np.array([w2v_model.wv[word] for word in words])\n",
    "\n",
    "    # Apply t-SNE to reduce to 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=\"steelblue\", alpha=0.7)\n",
    "\n",
    "    # Label points\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(\n",
    "            word, (embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=12, alpha=0.8\n",
    "        )\n",
    "\n",
    "    plt.title(\"Word Embeddings Visualization (t-SNE)\", fontsize=15)\n",
    "    plt.xlabel(\"t-SNE Dimension 1\", fontsize=12)\n",
    "    plt.ylabel(\"t-SNE Dimension 2\", fontsize=12)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not visualize word embeddings: {e}\")\n",
    "    print(\"Try with a larger dataset to build a more robust vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 6. Named Entity Recognition for Governance Text\n",
    "\n",
    "Extract political entities using our specialized NER function.\n",
    "\"\"\"\n",
    "\n",
    "# We'll use spaCy's NER capabilities\n",
    "import spacy\n",
    "\n",
    "try:\n",
    "    # Try to load the spaCy model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Extract political entities from our texts\n",
    "    political_entities = feature_extraction.extract_political_entities(\n",
    "        df[\"text\"].tolist()[:3], nlp\n",
    "    )\n",
    "\n",
    "    # Display entities found\n",
    "    for i, (text, entities) in enumerate(\n",
    "        zip(df[\"text\"].tolist()[:3], political_entities)\n",
    "    ):\n",
    "        print(f\"\\nText {i+1}: {text}\")\n",
    "        print(\"Entities found:\")\n",
    "        for entity in entities:\n",
    "            print(f\"- {entity['text']} ({entity['type']})\")\n",
    "\n",
    "    # Count entities by type\n",
    "    entity_types = {}\n",
    "    for doc_entities in political_entities:\n",
    "        for entity in doc_entities:\n",
    "            entity_type = entity[\"type\"]\n",
    "            entity_types[entity_type] = entity_types.get(entity_type, 0) + 1\n",
    "\n",
    "    # Plot entity distribution\n",
    "    if entity_types:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(entity_types.keys(), entity_types.values(), color=\"steelblue\")\n",
    "        plt.title(\"Political Entity Types in Sample Texts\")\n",
    "        plt.xlabel(\"Entity Type\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No political entities found in the sample texts.\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\n",
    "        \"SpaCy model 'en_core_web_sm' not available. Install with: python -m spacy download en_core_web_sm\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error during entity extraction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 7. Data Augmentation for Governance Text\n",
    "\n",
    "Apply our specialized data augmentation techniques for governance text.\n",
    "\"\"\"\n",
    "\n",
    "# Sample text for augmentation demonstration\n",
    "sample_text = df[\"text\"].iloc[0]\n",
    "print(f\"Original text: {sample_text}\\n\")\n",
    "\n",
    "# Apply various augmentation techniques\n",
    "print(\"Data augmentation examples:\\n\")\n",
    "\n",
    "# 1. Synonym replacement\n",
    "augmented = data_augmentation.synonym_replacement(sample_text.split(), n=2)\n",
    "print(f\"Synonym replacement: {' '.join(augmented)}\\n\")\n",
    "\n",
    "# 2. Random swap\n",
    "augmented = data_augmentation.random_swap(sample_text.split(), n=2)\n",
    "print(f\"Random swap: {' '.join(augmented)}\\n\")\n",
    "\n",
    "# 3. Random deletion\n",
    "augmented = data_augmentation.random_deletion(sample_text.split(), p=0.1)\n",
    "print(f\"Random deletion: {' '.join(augmented)}\\n\")\n",
    "\n",
    "# 4. Random insertion\n",
    "augmented = data_augmentation.random_insertion(sample_text.split(), n=2)\n",
    "print(f\"Random insertion: {' '.join(augmented)}\\n\")\n",
    "\n",
    "# 5. Governance-specific augmentation\n",
    "augmented = data_augmentation.governance_specific_augmentation(sample_text)\n",
    "print(f\"Governance-specific: {augmented}\\n\")\n",
    "\n",
    "# Create multiple augmented examples\n",
    "augmented_examples = data_augmentation.create_augmented_examples(\n",
    "    sample_text, n_examples=3\n",
    ")\n",
    "print(\"Multiple augmentation examples:\")\n",
    "for i, example in enumerate(augmented_examples):\n",
    "    print(f\"{i+1}: {example}\")\n",
    "\n",
    "# Demonstrate dataset balancing with augmentation\n",
    "# First, create an imbalanced dataset\n",
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter(df[\"label\"])\n",
    "print(\"\\nLabel distribution before balancing:\")\n",
    "print(label_counts)\n",
    "\n",
    "# Balance the dataset\n",
    "balanced_texts, balanced_labels = data_augmentation.balance_dataset(\n",
    "    df[\"text\"].tolist(), df[\"label\"].tolist()\n",
    ")\n",
    "\n",
    "balanced_counts = Counter(balanced_labels)\n",
    "print(\"\\nLabel distribution after balancing:\")\n",
    "print(balanced_counts)\n",
    "\n",
    "# Create a DataFrame with balanced data\n",
    "balanced_df = pd.DataFrame({\"text\": balanced_texts, \"label\": balanced_labels})\n",
    "\n",
    "# Plot the label distribution before and after balancing\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(label_counts.keys(), label_counts.values(), color=\"steelblue\")\n",
    "plt.title(\"Label Distribution Before Balancing\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(balanced_counts.keys(), balanced_counts.values(), color=\"steelblue\")\n",
    "plt.title(\"Label Distribution After Balancing\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 8. Complete Preprocessing Pipeline\n",
    "\n",
    "Put everything together into a comprehensive preprocessing pipeline for governance text.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def preprocess_governance_text(texts, labels=None, mode=\"train\"):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for governance text analysis.\n",
    "\n",
    "    Args:\n",
    "        texts: List of raw texts\n",
    "        labels: Optional list of labels\n",
    "        mode: 'train' or 'predict' mode\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with preprocessed features and metadata\n",
    "    \"\"\"\n",
    "    print(f\"Starting preprocessing pipeline in {mode} mode...\")\n",
    "    results = {\"pipeline_steps\": []}\n",
    "\n",
    "    # Step 1: Text cleaning\n",
    "    print(\"Step 1: Text cleaning...\")\n",
    "    cleaned_texts = text_cleaning.clean_text_batch(texts)\n",
    "    results[\"cleaned_texts\"] = cleaned_texts\n",
    "    results[\"pipeline_steps\"].append(\"text_cleaning\")\n",
    "\n",
    "    # Step 2: Tokenization\n",
    "    print(\"Step 2: Governance-specific tokenization...\")\n",
    "    tokenized_texts = [\n",
    "        tokenization.custom_governance_tokenizer(text) for text in cleaned_texts\n",
    "    ]\n",
    "    results[\"tokenized_texts\"] = tokenized_texts\n",
    "    results[\"pipeline_steps\"].append(\"tokenization\")\n",
    "\n",
    "    # Step 3: Feature extraction\n",
    "    print(\"Step 3: Feature extraction...\")\n",
    "\n",
    "    # 3.1: TF-IDF features\n",
    "    tfidf_features, tfidf_vectorizer = feature_extraction.create_tfidf_features(\n",
    "        cleaned_texts,\n",
    "        **config.DEFAULT_FEATURE_EXTRACTION_CONFIG.get(\"tfidf_params\", {}),\n",
    "    )\n",
    "    results[\"tfidf_features\"] = tfidf_features\n",
    "    results[\"tfidf_vectorizer\"] = tfidf_vectorizer\n",
    "\n",
    "    # 3.2: Word embeddings (if in training mode)\n",
    "    if mode == \"train\":\n",
    "        w2v_model = feature_extraction.create_word2vec_embeddings(\n",
    "            tokenized_texts,\n",
    "            **config.DEFAULT_FEATURE_EXTRACTION_CONFIG.get(\"word2vec_params\", {}),\n",
    "        )\n",
    "        results[\"word2vec_model\"] = w2v_model\n",
    "\n",
    "        # Create document embeddings\n",
    "        word_vectors = {word: w2v_model.wv[word] for word in w2v_model.wv.index_to_key}\n",
    "        doc_embeddings = feature_extraction.create_document_embeddings(\n",
    "            tokenized_texts, word_vectors\n",
    "        )\n",
    "        results[\"doc_embeddings\"] = doc_embeddings\n",
    "\n",
    "    results[\"pipeline_steps\"].append(\"feature_extraction\")\n",
    "\n",
    "    # Step 4: Named entity recognition (optional)\n",
    "    try:\n",
    "        print(\"Step 4: Political entity extraction...\")\n",
    "        import spacy\n",
    "\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        political_entities = feature_extraction.extract_political_entities(texts, nlp)\n",
    "        results[\"political_entities\"] = political_entities\n",
    "        results[\"pipeline_steps\"].append(\"entity_extraction\")\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping entity extraction: {e}\")\n",
    "\n",
    "    # Step 5: Data balancing and augmentation (only in training mode)\n",
    "    if mode == \"train\" and labels is not None:\n",
    "        print(\"Step 5: Dataset balancing and augmentation...\")\n",
    "        try:\n",
    "            augmented_texts, augmented_labels = data_augmentation.balance_dataset(\n",
    "                texts, labels\n",
    "            )\n",
    "            results[\"augmented_texts\"] = augmented_texts\n",
    "            results[\"augmented_labels\"] = augmented_labels\n",
    "            results[\"pipeline_steps\"].append(\"data_augmentation\")\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping data augmentation: {e}\")\n",
    "\n",
    "    print(\"Preprocessing pipeline completed successfully.\")\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run the complete pipeline on our sample dataset\n",
    "pipeline_results = preprocess_governance_text(\n",
    "    df[\"text\"].tolist(), df[\"label\"].tolist(), mode=\"train\"\n",
    ")\n",
    "\n",
    "# Show available results\n",
    "print(\"\\nPipeline results summary:\")\n",
    "for key in pipeline_results:\n",
    "    if key == \"pipeline_steps\":\n",
    "        print(f\"{key}: {pipeline_results[key]}\")\n",
    "    elif isinstance(pipeline_results[key], list):\n",
    "        print(f\"{key}: List with {len(pipeline_results[key])} items\")\n",
    "    elif hasattr(pipeline_results[key], \"shape\"):\n",
    "        print(f\"{key}: Array/Matrix with shape {pipeline_results[key].shape}\")\n",
    "    else:\n",
    "        print(f\"{key}: {type(pipeline_results[key])}\")\n",
    "\n",
    "# Use the preprocessed features for model training (placeholder)\n",
    "print(\"\\nThe preprocessed features can now be used for model training.\")\n",
    "print(\"For example, using TF-IDF features:\")\n",
    "print(\n",
    "    f\"X = pipeline_results['tfidf_features']  # Shape: {pipeline_results['tfidf_features'].shape}\"\n",
    ")\n",
    "print(\n",
    "    \"y = pipeline_results['augmented_labels'] if 'augmented_labels' in pipeline_results else df['label']\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 9. Saving and Loading Pipeline Components\n",
    "\n",
    "Demonstrate how to save and load preprocessing components for later use.\n",
    "This is essential for maintaining consistency between training and inference.\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create directories for saving pipeline components\n",
    "models_dir = Path(\"../models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "preprocessing_dir = models_dir / \"preprocessing\"\n",
    "preprocessing_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save components\n",
    "\n",
    "\n",
    "def save_pipeline_components(results, output_dir):\n",
    "    \"\"\"Save preprocessing pipeline components.\"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Save vectorizers\n",
    "    if \"tfidf_vectorizer\" in results:\n",
    "        with open(output_dir / \"tfidf_vectorizer.pkl\", \"wb\") as f:\n",
    "            pickle.dump(results[\"tfidf_vectorizer\"], f)\n",
    "\n",
    "    # Save Word2Vec model\n",
    "    if \"word2vec_model\" in results:\n",
    "        results[\"word2vec_model\"].save(str(output_dir / \"word2vec_model.w2v\"))\n",
    "\n",
    "    # Save preprocessing configuration\n",
    "    pipeline_config = {\n",
    "        \"steps\": results[\"pipeline_steps\"],\n",
    "        \"has_tfidf\": \"tfidf_vectorizer\" in results,\n",
    "        \"has_word2vec\": \"word2vec_model\" in results,\n",
    "    }\n",
    "\n",
    "    with open(output_dir / \"pipeline_config.json\", \"w\") as f:\n",
    "        json.dump(pipeline_config, f, indent=2)\n",
    "\n",
    "    print(f\"Pipeline components saved to {output_dir}\")\n",
    "\n",
    "\n",
    "# Example of saving pipeline components\n",
    "save_pipeline_components(pipeline_results, preprocessing_dir)\n",
    "\n",
    "# Function for loading pipeline components\n",
    "\n",
    "\n",
    "def load_pipeline_components(input_dir):\n",
    "    \"\"\"Load preprocessing pipeline components.\"\"\"\n",
    "    input_dir = Path(input_dir)\n",
    "    results = {}\n",
    "\n",
    "    # Load pipeline configuration\n",
    "    try:\n",
    "        with open(input_dir / \"pipeline_config.json\", \"r\") as f:\n",
    "            pipeline_config = json.load(f)\n",
    "            results[\"pipeline_steps\"] = pipeline_config[\"steps\"]\n",
    "    except FileNotFoundError:\n",
    "        print(\"Pipeline configuration not found.\")\n",
    "        return {}\n",
    "\n",
    "    # Load TF-IDF vectorizer if available\n",
    "    if pipeline_config.get(\"has_tfidf\", False):\n",
    "        try:\n",
    "            with open(input_dir / \"tfidf_vectorizer.pkl\", \"rb\") as f:\n",
    "                results[\"tfidf_vectorizer\"] = pickle.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(\"TF-IDF vectorizer not found.\")\n",
    "\n",
    "    # Load Word2Vec model if available\n",
    "    if pipeline_config.get(\"has_word2vec\", False):\n",
    "        try:\n",
    "            from gensim.models import Word2Vec\n",
    "\n",
    "            results[\"word2vec_model\"] = Word2Vec.load(\n",
    "                str(input_dir / \"word2vec_model.w2v\")\n",
    "            )\n",
    "        except FileNotFoundError:\n",
    "            print(\"Word2Vec model not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Word2Vec model: {e}\")\n",
    "\n",
    "    print(f\"Pipeline components loaded from {input_dir}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example of loading pipeline components\n",
    "loaded_components = load_pipeline_components(preprocessing_dir)\n",
    "print(\"\\nLoaded components:\")\n",
    "for key in loaded_components:\n",
    "    print(f\"- {key}\")\n",
    "\n",
    "# Demonstrate how to use loaded components for inference\n",
    "\n",
    "\n",
    "def preprocess_for_inference(text, loaded_components):\n",
    "    \"\"\"Apply preprocessing pipeline to new text using saved components.\"\"\"\n",
    "    if not loaded_components:\n",
    "        print(\"No pipeline components loaded.\")\n",
    "        return None\n",
    "\n",
    "    # Clean text\n",
    "    cleaned_text = text_cleaning.clean_governance_text(text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = tokenization.custom_governance_tokenizer(cleaned_text)\n",
    "\n",
    "    # Extract features\n",
    "    features = {}\n",
    "\n",
    "    # TF-IDF features\n",
    "    if \"tfidf_vectorizer\" in loaded_components:\n",
    "        tfidf_features = loaded_components[\"tfidf_vectorizer\"].transform([cleaned_text])\n",
    "        features[\"tfidf\"] = tfidf_features\n",
    "\n",
    "    # Word2Vec features\n",
    "    if \"word2vec_model\" in loaded_components:\n",
    "        w2v_model = loaded_components[\"word2vec_model\"]\n",
    "\n",
    "        # Create document embedding\n",
    "        word_vectors = {word: w2v_model.wv[word] for word in w2v_model.wv.index_to_key}\n",
    "        doc_embedding = feature_extraction.document_embedding_average(\n",
    "            tokens, word_vectors\n",
    "        )\n",
    "\n",
    "        if len(doc_embedding) > 0:\n",
    "            features[\"word2vec\"] = doc_embedding\n",
    "\n",
    "    return {\"cleaned_text\": cleaned_text, \"tokens\": tokens, \"features\": features}\n",
    "\n",
    "\n",
    "# Example new text for inference\n",
    "new_text = \"Electoral reform is essential for strengthening democratic institutions.\"\n",
    "processed = preprocess_for_inference(new_text, loaded_components)\n",
    "\n",
    "print(\"\\nPreprocessed new text for inference:\")\n",
    "print(f\"Original: {new_text}\")\n",
    "print(f\"Cleaned: {processed['cleaned_text']}\")\n",
    "print(f\"Tokens: {processed['tokens']}\")\n",
    "print(\"Features extracted:\")\n",
    "for feat_type, feat in processed[\"features\"].items():\n",
    "    if hasattr(feat, \"shape\"):\n",
    "        print(f\"- {feat_type}: Shape {feat.shape}\")\n",
    "    else:\n",
    "        print(f\"- {feat_type}: Length {len(feat)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## 10. Conclusion and Next Steps\n",
    "\n",
    "This notebook has demonstrated the complete preprocessing pipeline for governance text analysis,\n",
    "which will serve as a foundation for our sentiment analysis model and other NLP tasks.\n",
    "\n",
    "### Summary of what we've covered:\n",
    "1. Text cleaning specialized for governance text\n",
    "2. Domain-specific tokenization preserving governance phrases\n",
    "3. Feature extraction using traditional approaches (BoW, TF-IDF) and word embeddings\n",
    "4. Political entity recognition for governance text\n",
    "5. Data augmentation techniques specific to governance domain\n",
    "6. Dataset balancing for handling imbalanced class distributions\n",
    "7. Complete preprocessing pipeline implementation\n",
    "8. Saving and loading pipeline components for inference\n",
    "\n",
    "### Next steps:\n",
    "1. Connect this preprocessing pipeline to the sentiment analysis model\n",
    "2. Apply these preprocessing techniques to the full Austria Democracy Radar dataset\n",
    "3. Extend the preprocessing pipeline with language detection and filtering\n",
    "4. Refine governance-specific text augmentation techniques\n",
    "5. Integrate the preprocessing pipeline into an end-to-end API service\n",
    "\"\"\"\n",
    "\n",
    "# Run some final checks\n",
    "print(\"Preprocessing modules ready for integration into sentiment analysis pipeline.\")\n",
    "print(\n",
    "    f\"Text cleaning functions: {len([f for f in dir(text_cleaning) if callable(getattr(text_cleaning, f)) and not f.startswith('_')])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Tokenization functions: {len([f for f in dir(tokenization) if callable(getattr(tokenization, f)) and not f.startswith('_')])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Feature extraction functions: {len([f for f in dir(feature_extraction) if callable(getattr(feature_extraction, f)) and not f.startswith('_')])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Data augmentation functions: {len([f for f in dir(data_augmentation) if callable(getattr(data_augmentation, f)) and not f.startswith('_')])}\"\n",
    ")\n",
    "\n",
    "print(\"\\nEnd of preprocessing pipeline demonstration.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
