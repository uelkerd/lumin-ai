{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LUMIN.AI: Sentiment Analysis MVP\n",
    "\n",
    "This notebook implements a minimum viable product for sentiment analysis in the LUMIN.AI project, specifically focused on analyzing governance-related texts. According to our roadmap, this is a key deliverable for Week 2, where we build a basic sentiment classification model.\n",
    "\n",
    "## Objectives\n",
    "- Implement a basic sentiment analysis pipeline for governance texts\n",
    "- Compare traditional ML approaches with neural network methods\n",
    "- Evaluate model performance on governance-specific language\n",
    "- Create a foundation for the production-ready API service\n",
    "\n",
    "## Success Criteria\n",
    "- Model achieves at least 70% accuracy on governance text sentiment classification\n",
    "- Clear documentation of the preprocessing and model training process\n",
    "- Reproducible results for future development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import warnings\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For text preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "# For machine learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# For neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import pipeline\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"viridis\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "    nltk.data.find(\"corpora/wordnet\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"stopwords\")\n",
    "    nltk.download(\"wordnet\")\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading spaCy model...\")\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Data Loading\n",
    "\n",
    "In this section, we'll load the sample data and prepare it for sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "SAMPLE_DATA_PATH = \"../../data/examples/sample_data.csv\"\n",
    "FULL_DATA_PATH = \"../../data/raw/democracy-radar/\"\n",
    "MODEL_SAVE_PATH = \"../models/\"\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "# Check if sample data exists and load it\n",
    "if os.path.exists(SAMPLE_DATA_PATH):\n",
    "    print(f\"Loading sample data from {SAMPLE_DATA_PATH}\")\n",
    "    data = pd.read_csv(SAMPLE_DATA_PATH)\n",
    "    print(f\"Sample data loaded successfully with {len(data)} rows\")\n",
    "else:\n",
    "    print(f\"Sample data not found at {SAMPLE_DATA_PATH}\")\n",
    "    print(\"Creating synthetic data for development\")\n",
    "    # Create synthetic data with governance-related text\n",
    "    governance_texts = [\n",
    "        \"This governance proposal is excellent and transparent.\",\n",
    "        \"I don't trust this process at all.\",\n",
    "        \"The proposal contains both good and bad elements.\",\n",
    "        \"Very clear communication from the team.\",\n",
    "        \"Too many decisions made behind closed doors.\",\n",
    "        \"The voting process was fair and accessible.\",\n",
    "        \"The documentation is insufficient.\",\n",
    "        \"Waiting to see how implementation goes before judging.\",\n",
    "        \"Great to see community feedback incorporated.\",\n",
    "        \"Timeline is unrealistic and poorly planned.\",\n",
    "    ]\n",
    "    sentiments = [\n",
    "        \"positive\",\n",
    "        \"negative\",\n",
    "        \"neutral\",\n",
    "        \"positive\",\n",
    "        \"negative\",\n",
    "        \"positive\",\n",
    "        \"negative\",\n",
    "        \"neutral\",\n",
    "        \"positive\",\n",
    "        \"negative\",\n",
    "    ]\n",
    "    categories = [\n",
    "        \"transparency\",\n",
    "        \"trust\",\n",
    "        \"evaluation\",\n",
    "        \"communication\",\n",
    "        \"transparency\",\n",
    "        \"participation\",\n",
    "        \"documentation\",\n",
    "        \"implementation\",\n",
    "        \"feedback\",\n",
    "        \"planning\",\n",
    "    ]\n",
    "    confidence = [0.92, 0.87, 0.76, 0.95, 0.89, 0.91, 0.82, 0.79, 0.93, 0.88]\n",
    "\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": range(1, 11),\n",
    "            \"text\": governance_texts,\n",
    "            \"sentiment\": sentiments,\n",
    "            \"category\": categories,\n",
    "            \"confidence\": confidence,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Display the first few rows of the data\n",
    "print(\"\\nFirst 5 rows of the data:\")\n",
    "display(data.head())\n",
    "\n",
    "# Check sentiment distribution\n",
    "print(\"\\nSentiment distribution:\")\n",
    "display(data[\"sentiment\"].value_counts())\n",
    "\n",
    "# Check category distribution\n",
    "print(\"\\nCategory distribution:\")\n",
    "display(data[\"category\"].value_counts())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "In this section, we'll preprocess the text data to prepare it for sentiment analysis. This includes:\n",
    "- Converting text to lowercase\n",
    "- Removing special characters and numbers\n",
    "- Tokenization\n",
    "- Removing stopwords\n",
    "- Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text for sentiment analysis:\n",
    "    1. Convert to lowercase\n",
    "    2. Remove special characters and numbers\n",
    "    3. Remove extra whitespace\n",
    "    4. Tokenize\n",
    "    5. Remove stopwords\n",
    "    6. Lemmatize\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Join tokens back into text\n",
    "    processed_text = \" \".join(tokens)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "# Apply preprocessing to text column\n",
    "data[\"processed_text\"] = data[\"text\"].apply(preprocess_text)\n",
    "\n",
    "# Display examples of original and preprocessed text\n",
    "print(\"Examples of original and preprocessed text:\")\n",
    "text_examples = pd.DataFrame(\n",
    "    {\n",
    "        \"Original Text\": data[\"text\"].head(),\n",
    "        \"Preprocessed Text\": data[\"processed_text\"].head(),\n",
    "    }\n",
    ")\n",
    "display(text_examples)\n",
    "\n",
    "# Check for empty processed texts\n",
    "empty_texts = data[data[\"processed_text\"] == \"\"]\n",
    "if len(empty_texts) > 0:\n",
    "    print(f\"\\nWarning: {len(empty_texts)} texts were empty after preprocessing.\")\n",
    "    display(empty_texts)\n",
    "else:\n",
    "    print(\"\\nAll texts were successfully preprocessed.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Feature Extraction\n",
    "\n",
    "In this section, we'll convert the preprocessed text into numerical features that can be used by machine learning algorithms. We'll compare two common approaches:\n",
    "- Bag of Words (CountVectorizer)\n",
    "- TF-IDF (Term Frequency-Inverse Document Frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "X = data[\"processed_text\"]\n",
    "y = data[\"sentiment\"]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n",
    "\n",
    "# Create a CountVectorizer (Bag of Words)\n",
    "count_vectorizer = CountVectorizer(max_features=1000)\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "# Display the vocabulary size and sample features\n",
    "print(f\"\\nBag of Words vocabulary size: {len(count_vectorizer.vocabulary_)}\")\n",
    "print(\"Sample features (first 10):\", list(count_vectorizer.vocabulary_.keys())[:10])\n",
    "\n",
    "# Create a TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Display the vocabulary size and sample features\n",
    "print(f\"\\nTF-IDF vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "print(\"Sample features (first 10):\", list(tfidf_vectorizer.vocabulary_.keys())[:10])\n",
    "\n",
    "# Compare the feature representations\n",
    "print(\"\\nBag of Words representation (first sample):\")\n",
    "print(X_train_counts[0].toarray())\n",
    "\n",
    "print(\"\\nTF-IDF representation (first sample):\")\n",
    "print(X_train_tfidf[0].toarray())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Traditional Machine Learning Models\n",
    "\n",
    "In this section, we'll train and evaluate several traditional machine learning models for sentiment analysis:\n",
    "- Naive Bayes\n",
    "- Logistic Regression\n",
    "- Support Vector Machine (SVM)\n",
    "- Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to evaluate\n",
    "models = {\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"SVM\": LinearSVC(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "}\n",
    "\n",
    "# Define a function to evaluate models\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"{model_name} Performance:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=sorted(data[\"sentiment\"].unique()),\n",
    "        yticklabels=sorted(data[\"sentiment\"].unique()),\n",
    "    )\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.show()\n",
    "\n",
    "    return model, accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "# Evaluate models using Bag of Words features\n",
    "print(\"Evaluating models with Bag of Words features...\\n\")\n",
    "bow_results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    model, accuracy, precision, recall, f1 = evaluate_model(\n",
    "        model, X_train_counts, X_test_counts, y_train, y_test, f\"{name} (BoW)\"\n",
    "    )\n",
    "    bow_results.append(\n",
    "        {\n",
    "            \"Model\": f\"{name} (BoW)\",\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Evaluate models using TF-IDF features\n",
    "print(\"\\nEvaluating models with TF-IDF features...\\n\")\n",
    "tfidf_results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    model, accuracy, precision, recall, f1 = evaluate_model(\n",
    "        model, X_train_tfidf, X_test_tfidf, y_train, y_test, f\"{name} (TF-IDF)\"\n",
    "    )\n",
    "    tfidf_results.append(\n",
    "        {\n",
    "            \"Model\": f\"{name} (TF-IDF)\",\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Combine results\n",
    "all_results = pd.DataFrame(bow_results + tfidf_results)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "display(all_results.sort_values(\"F1 Score\", ascending=False))\n",
    "\n",
    "# Plot model comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x=\"Model\", y=\"Accuracy\", data=all_results, palette=\"viridis\")\n",
    "plt.title(\"Model Accuracy Comparison\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify the best model\n",
    "best_model_row = all_results.loc[all_results[\"F1 Score\"].idxmax()]\n",
    "best_model_name = best_model_row[\"Model\"]\n",
    "print(f\"\\nBest model: {best_model_name}\")\n",
    "print(f\"Accuracy: {best_model_row['Accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {best_model_row['F1 Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Model Optimization\n",
    "\n",
    "In this section, we'll optimize the best performing model using hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which model and feature extraction method to optimize\n",
    "# For demonstration, let's assume SVM with TF-IDF performed best\n",
    "# In a real scenario, you would use the best_model_name variable to determine this\n",
    "\n",
    "# Create a pipeline for the model\n",
    "pipeline = Pipeline(\n",
    "    [(\"vectorizer\", TfidfVectorizer()), (\"classifier\", LinearSVC(random_state=42))]\n",
    ")\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    \"vectorizer__max_features\": [500, 1000, 2000],\n",
    "    \"vectorizer__ngram_range\": [(1, 1), (1, 2)],  # Unigrams or unigrams+bigrams\n",
    "    \"classifier__C\": [0.1, 1.0, 10.0],  # Regularization parameter\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "print(\"Performing grid search with cross-validation...\")\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, param_grid, cv=5, scoring=\"f1_weighted\", n_jobs=-1, verbose=1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"\\nBest parameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Evaluate the optimized model\n",
    "optimized_model = grid_search.best_estimator_\n",
    "y_pred = optimized_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "print(\"\\nOptimized Model Performance:\")\n",
    "print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall: {recall:.4f}\")\n",
    "print(f\"  F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=sorted(data[\"sentiment\"].unique()),\n",
    "    yticklabels=sorted(data[\"sentiment\"].unique()),\n",
    ")\n",
    "plt.title(\"Confusion Matrix - Optimized Model\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()\n",
    "\n",
    "# Save the optimized model\n",
    "optimized_model_path = os.path.join(MODEL_SAVE_PATH, \"optimized_sentiment_model.pkl\")\n",
    "with open(optimized_model_path, \"wb\") as f:\n",
    "    pickle.dump(optimized_model, f)\n",
    "\n",
    "print(f\"\\nOptimized model saved to {optimized_model_path}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Transformer-Based Sentiment Analysis\n",
    "\n",
    "In this section, we'll use a pre-trained transformer model for sentiment analysis and compare its performance with our traditional machine learning approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the sentiment analysis pipeline using a pre-trained model\n",
    "print(\"Setting up transformer-based sentiment analysis pipeline...\")\n",
    "transformer_sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Function to map Hugging Face sentiment labels to our labels\n",
    "\n",
    "\n",
    "def map_sentiment(result):\n",
    "    # Hugging Face returns 'POSITIVE' or 'NEGATIVE'\n",
    "    # We need to map to 'positive', 'negative', or 'neutral'\n",
    "    label = result[0][\"label\"].lower()\n",
    "    score = result[0][\"score\"]\n",
    "\n",
    "    # If the score is close to 0.5, consider it neutral\n",
    "    if 0.4 <= score <= 0.6:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return label\n",
    "\n",
    "\n",
    "# Test the transformer model on a few examples\n",
    "print(\"\\nTesting transformer model on example texts:\")\n",
    "for i, text in enumerate(data[\"text\"].head(5)):\n",
    "    result = transformer_sentiment_analyzer(text)\n",
    "    mapped_sentiment = map_sentiment(result)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"True sentiment: {data['sentiment'].iloc[i]}\")\n",
    "    print(\n",
    "        f\"Transformer prediction: {mapped_sentiment} (raw: {result[0]['label']} with score: {result[0]['score']:.4f})\"\n",
    "    )\n",
    "\n",
    "# Evaluate the transformer model on the test set\n",
    "print(\"\\nEvaluating transformer model on test set...\")\n",
    "transformer_predictions = []\n",
    "\n",
    "for text in tqdm(X_test):\n",
    "    result = transformer_sentiment_analyzer(text)\n",
    "    mapped_sentiment = map_sentiment(result)\n",
    "    transformer_predictions.append(mapped_sentiment)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, transformer_predictions)\n",
    "precision = precision_score(y_test, transformer_predictions, average=\"weighted\")\n",
    "recall = recall_score(y_test, transformer_predictions, average=\"weighted\")\n",
    "f1 = f1_score(y_test, transformer_predictions, average=\"weighted\")\n",
    "\n",
    "print(\"\\nTransformer Model Performance:\")\n",
    "print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall: {recall:.4f}\")\n",
    "print(f\"  F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, transformer_predictions))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, transformer_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=sorted(data[\"sentiment\"].unique()),\n",
    "    yticklabels=sorted(data[\"sentiment\"].unique()),\n",
    ")\n",
    "plt.title(\"Confusion Matrix - Transformer Model\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Model Deployment and API Integration\n",
    "\n",
    "In this section, we'll demonstrate how to use the trained model for inference and prepare it for API integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to predict sentiment using our optimized model\n",
    "def predict_sentiment_ml(text, model=optimized_model):\n",
    "    \"\"\"\n",
    "    Predict sentiment using the optimized machine learning model.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text to analyze\n",
    "        model: Trained sentiment analysis model\n",
    "\n",
    "    Returns:\n",
    "        dict: Prediction results with sentiment label and confidence score\n",
    "    \"\"\"\n",
    "    # Preprocess the text\n",
    "    processed_text = preprocess_text(text)\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = model.predict([processed_text])[0]\n",
    "\n",
    "    # Get probability scores if the model supports it\n",
    "    try:\n",
    "        proba = model.predict_proba([processed_text])[0]\n",
    "        confidence = proba.max()\n",
    "    except:\n",
    "        # For models that don't support predict_proba (like LinearSVC)\n",
    "        confidence = 0.8  # Default confidence\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"sentiment\": prediction,\n",
    "        \"confidence\": float(confidence),\n",
    "        \"model_type\": \"machine_learning\",\n",
    "    }\n",
    "\n",
    "\n",
    "# Create a function to predict sentiment using the transformer model\n",
    "\n",
    "\n",
    "def predict_sentiment_transformer(text, model=transformer_sentiment_analyzer):\n",
    "    \"\"\"\n",
    "    Predict sentiment using the transformer model.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text to analyze\n",
    "        model: Transformer sentiment analysis pipeline\n",
    "\n",
    "    Returns:\n",
    "        dict: Prediction results with sentiment label and confidence score\n",
    "    \"\"\"\n",
    "    # Make prediction\n",
    "    result = model(text)\n",
    "    label = result[0][\"label\"].lower()\n",
    "    score = result[0][\"score\"]\n",
    "\n",
    "    # Map to our sentiment categories\n",
    "    if 0.4 <= score <= 0.6:\n",
    "        sentiment = \"neutral\"\n",
    "    else:\n",
    "        sentiment = label\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"sentiment\": sentiment,\n",
    "        \"confidence\": float(score),\n",
    "        \"model_type\": \"transformer\",\n",
    "    }\n",
    "\n",
    "\n",
    "# Test both models on new examples\n",
    "test_texts = [\n",
    "    \"The government's transparency initiative has been a great success.\",\n",
    "    \"Citizens are increasingly frustrated with the lack of accountability.\",\n",
    "    \"The new policy has both advantages and disadvantages that need careful consideration.\",\n",
    "    \"Public trust in institutions remains stable according to recent surveys.\",\n",
    "]\n",
    "\n",
    "print(\"Predictions using optimized machine learning model:\")\n",
    "for text in test_texts:\n",
    "    result = predict_sentiment_ml(text)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Sentiment: {result['sentiment']} (confidence: {result['confidence']:.4f})\")\n",
    "\n",
    "print(\"\\nPredictions using transformer model:\")\n",
    "for text in test_texts:\n",
    "    result = predict_sentiment_transformer(text)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Sentiment: {result['sentiment']} (confidence: {result['confidence']:.4f})\")\n",
    "\n",
    "# Example of how to integrate with a FastAPI endpoint\n",
    "print(\"\\nExample FastAPI endpoint code:\")\n",
    "print(\n",
    "    \"\"\"\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import pickle\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load the model\n",
    "with open('models/optimized_sentiment_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "class TextRequest(BaseModel):\n",
    "    text: str\n",
    "\n",
    "class SentimentResponse(BaseModel):\n",
    "    text: str\n",
    "    sentiment: str\n",
    "    confidence: float\n",
    "    model_type: str\n",
    "\n",
    "@app.post(\"/predict\", response_model=SentimentResponse)\n",
    "async def predict_sentiment(request: TextRequest):\n",
    "    try:\n",
    "        result = predict_sentiment_ml(request.text, model)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've built a minimum viable product for sentiment analysis of governance-related texts. We've:\n",
    "\n",
    "1. Loaded and preprocessed text data\n",
    "2. Extracted features using both Bag of Words and TF-IDF approaches\n",
    "3. Trained and evaluated traditional machine learning models\n",
    "4. Optimized the best-performing model using hyperparameter tuning\n",
    "5. Compared our model with a pre-trained transformer model\n",
    "6. Demonstrated how to deploy the model as an API endpoint\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Fine-tune the transformer model**: Train the transformer model specifically on governance-related texts\n",
    "2. **Expand the dataset**: Collect more governance-specific text data for training\n",
    "3. **Add more sentiment categories**: Go beyond positive/negative/neutral to include more nuanced sentiments\n",
    "4. **Implement aspect-based sentiment analysis**: Analyze sentiment toward specific aspects of governance\n",
    "5. **Deploy the model**: Create a production-ready API service\n",
    "6. **Monitor model performance**: Set up tracking of model performance in production\n",
    "7. **Integrate with the frontend**: Connect the sentiment analysis API with the web dashboard\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
