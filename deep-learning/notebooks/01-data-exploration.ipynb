{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LUMIN.AI: Data Exploration\n",
    "\n",
    "This notebook contains exploratory data analysis for the LUMIN.AI project's Deep Learning track. It focuses on exploring the Austria Democracy Radar dataset and preparing it for sentiment analysis.\n",
    "\n",
    "## Objectives\n",
    "- Set up the development environment with necessary libraries\n",
    "- Explore and understand the Austria Democracy Radar dataset structure\n",
    "- Perform initial data cleaning and preprocessing\n",
    "- Generate basic statistics and visualizations\n",
    "- Prepare the data for sentiment analysis modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "# For text preprocessing\n",
    "import spacy\n",
    "\n",
    "# For visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"viridis\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "    nltk.data.find(\"corpora/wordnet\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "    nltk.download(\"stopwords\")\n",
    "    nltk.download(\"wordnet\")\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading spaCy model...\")\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "In this section, we'll load the Austria Democracy Radar dataset. For now, we'll use the sample data provided in the repository, but instructions are included for downloading and using the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "SAMPLE_DATA_PATH = \"../../data/examples/sample_data.csv\"\n",
    "FULL_DATA_PATH = \"../../data/raw/democracy-radar/\"\n",
    "\n",
    "# Check if sample data exists and load it\n",
    "if os.path.exists(SAMPLE_DATA_PATH):\n",
    "    print(f\"Loading sample data from {SAMPLE_DATA_PATH}\")\n",
    "    data = pd.read_csv(SAMPLE_DATA_PATH)\n",
    "    print(f\"Sample data loaded successfully with {len(data)} rows\")\n",
    "else:\n",
    "    print(f\"Sample data not found at {SAMPLE_DATA_PATH}\")\n",
    "    print(\"Creating placeholder data for development\")\n",
    "    data = pd.DataFrame(np.random.randint(0, 100, size=(100, 4)), columns=list(\"ABCD\"))\n",
    "\n",
    "# Instructions for downloading the full dataset\n",
    "print(\n",
    "    \"\\nTo download the full Austria Democracy Radar dataset, run the following command:\"\n",
    ")\n",
    "print(\"python ../../data/scripts/download_data.py --dataset democracy-radar\")\n",
    "print(\"\\nAfter downloading, you can load the full dataset with:\")\n",
    "print(\"data = pd.read_csv('../../data/raw/democracy-radar/wave-1.csv')\")\n",
    "\n",
    "# Display the first few rows of the data\n",
    "print(\"\\nFirst 5 rows of the data:\")\n",
    "display(data.head())\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\nBasic information about the dataset:\")\n",
    "display(data.info())\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nSummary statistics:\")\n",
    "display(data.describe(include=\"all\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "In this section, we'll perform exploratory data analysis on the dataset to understand its structure, distribution, and identify any patterns or issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in each column:\")\n",
    "display(data.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = data.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicates}\")\n",
    "\n",
    "# Examine the distribution of sentiment labels\n",
    "if \"sentiment\" in data.columns:\n",
    "    print(\"\\nSentiment distribution:\")\n",
    "    sentiment_counts = data[\"sentiment\"].value_counts()\n",
    "    display(sentiment_counts)\n",
    "\n",
    "    # Visualize sentiment distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x=\"sentiment\", data=data, palette=\"viridis\")\n",
    "    plt.title(\"Distribution of Sentiment Labels\")\n",
    "    plt.xlabel(\"Sentiment\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "    # Pie chart of sentiment distribution\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.pie(\n",
    "        sentiment_counts,\n",
    "        labels=sentiment_counts.index,\n",
    "        autopct=\"%1.1f%%\",\n",
    "        startangle=90,\n",
    "        colors=sns.color_palette(\"viridis\", len(sentiment_counts)),\n",
    "    )\n",
    "    plt.title(\"Sentiment Distribution\")\n",
    "    plt.axis(\"equal\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the distribution of categories\n",
    "if \"category\" in data.columns:\n",
    "    print(\"\\nCategory distribution:\")\n",
    "    category_counts = data[\"category\"].value_counts()\n",
    "    display(category_counts)\n",
    "\n",
    "    # Visualize category distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(\n",
    "        y=\"category\", data=data, order=category_counts.index, palette=\"viridis\"\n",
    "    )\n",
    "    plt.title(\"Distribution of Categories\")\n",
    "    plt.xlabel(\"Count\")\n",
    "    plt.ylabel(\"Category\")\n",
    "    plt.show()\n",
    "\n",
    "    # Analyze sentiment distribution by category\n",
    "    if \"sentiment\" in data.columns:\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        sentiment_by_category = pd.crosstab(data[\"category\"], data[\"sentiment\"])\n",
    "        sentiment_by_category.plot(kind=\"bar\", stacked=True, colormap=\"viridis\")\n",
    "        plt.title(\"Sentiment Distribution by Category\")\n",
    "        plt.xlabel(\"Category\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.legend(title=\"Sentiment\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "\n",
    "        # Heatmap of sentiment by category\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sentiment_category_pivot = pd.crosstab(\n",
    "            data[\"category\"], data[\"sentiment\"], normalize=\"index\"\n",
    "        )\n",
    "        sns.heatmap(sentiment_category_pivot, annot=True, cmap=\"viridis\", fmt=\".2%\")\n",
    "        plt.title(\"Sentiment Proportion by Category\")\n",
    "        plt.xlabel(\"Sentiment\")\n",
    "        plt.ylabel(\"Category\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Text Analysis\n",
    "\n",
    "In this section, we'll analyze the text content of the dataset to prepare it for sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text statistics\n",
    "if \"text\" in data.columns:\n",
    "    # Calculate text length\n",
    "    data[\"text_length\"] = data[\"text\"].apply(len)\n",
    "\n",
    "    # Calculate word count\n",
    "    data[\"word_count\"] = data[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "    # Display text length statistics\n",
    "    print(\"Text length statistics:\")\n",
    "    display(data[[\"text_length\", \"word_count\"]].describe())\n",
    "\n",
    "    # Plot text length distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(data[\"text_length\"], kde=True, bins=30)\n",
    "    plt.title(\"Distribution of Text Length\")\n",
    "    plt.xlabel(\"Text Length (characters)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot word count distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(data[\"word_count\"], kde=True, bins=30)\n",
    "    plt.title(\"Distribution of Word Count\")\n",
    "    plt.xlabel(\"Word Count\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "    # Analyze text length by sentiment\n",
    "    if \"sentiment\" in data.columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(x=\"sentiment\", y=\"text_length\", data=data, palette=\"viridis\")\n",
    "        plt.title(\"Text Length by Sentiment\")\n",
    "        plt.xlabel(\"Sentiment\")\n",
    "        plt.ylabel(\"Text Length (characters)\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(x=\"sentiment\", y=\"word_count\", data=data, palette=\"viridis\")\n",
    "        plt.title(\"Word Count by Sentiment\")\n",
    "        plt.xlabel(\"Sentiment\")\n",
    "        plt.ylabel(\"Word Count\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing functions\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text for sentiment analysis:\n",
    "    1. Convert to lowercase\n",
    "    2. Remove special characters and numbers\n",
    "    3. Remove extra whitespace\n",
    "    4. Tokenize\n",
    "    5. Remove stopwords\n",
    "    6. Lemmatize\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Join tokens back into text\n",
    "    processed_text = \" \".join(tokens)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "# Apply preprocessing to text column\n",
    "if \"text\" in data.columns:\n",
    "    # Create a new column with preprocessed text\n",
    "    data[\"processed_text\"] = data[\"text\"].apply(preprocess_text)\n",
    "\n",
    "    # Display examples of original and preprocessed text\n",
    "    print(\"Examples of original and preprocessed text:\")\n",
    "    text_examples = pd.DataFrame(\n",
    "        {\n",
    "            \"Original Text\": data[\"text\"].head(),\n",
    "            \"Preprocessed Text\": data[\"processed_text\"].head(),\n",
    "        }\n",
    "    )\n",
    "    display(text_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency analysis\n",
    "if \"processed_text\" in data.columns:\n",
    "    # Combine all processed text\n",
    "    all_text = \" \".join(data[\"processed_text\"].tolist())\n",
    "\n",
    "    # Count word frequencies\n",
    "    words = all_text.split()\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    # Get the most common words\n",
    "    most_common_words = word_counts.most_common(20)\n",
    "\n",
    "    # Display most common words\n",
    "    print(\"Most common words in the dataset:\")\n",
    "    display(pd.DataFrame(most_common_words, columns=[\"Word\", \"Count\"]))\n",
    "\n",
    "    # Plot word frequency\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    common_words_df = pd.DataFrame(most_common_words, columns=[\"Word\", \"Count\"])\n",
    "    sns.barplot(x=\"Count\", y=\"Word\", data=common_words_df, palette=\"viridis\")\n",
    "    plt.title(\"Most Common Words\")\n",
    "    plt.xlabel(\"Count\")\n",
    "    plt.ylabel(\"Word\")\n",
    "    plt.show()\n",
    "\n",
    "    # Create word cloud\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=800,\n",
    "        background_color=\"white\",\n",
    "        colormap=\"viridis\",\n",
    "        max_words=100,\n",
    "    ).generate(all_text)\n",
    "\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Word Cloud of All Text\")\n",
    "    plt.show()\n",
    "\n",
    "    # Word frequency by sentiment\n",
    "    if \"sentiment\" in data.columns:\n",
    "        for sentiment in data[\"sentiment\"].unique():\n",
    "            # Get text for this sentiment\n",
    "            sentiment_text = \" \".join(\n",
    "                data[data[\"sentiment\"] == sentiment][\"processed_text\"].tolist()\n",
    "            )\n",
    "\n",
    "            # Count word frequencies\n",
    "            sentiment_words = sentiment_text.split()\n",
    "            sentiment_word_counts = Counter(sentiment_words)\n",
    "\n",
    "            # Get the most common words\n",
    "            sentiment_most_common = sentiment_word_counts.most_common(10)\n",
    "\n",
    "            # Display most common words for this sentiment\n",
    "            print(f\"\\nMost common words in {sentiment} sentiment:\")\n",
    "            display(pd.DataFrame(sentiment_most_common, columns=[\"Word\", \"Count\"]))\n",
    "\n",
    "            # Create word cloud for this sentiment\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            sentiment_wordcloud = WordCloud(\n",
    "                width=800,\n",
    "                height=800,\n",
    "                background_color=\"white\",\n",
    "                colormap=\"viridis\",\n",
    "                max_words=50,\n",
    "            ).generate(sentiment_text)\n",
    "\n",
    "            plt.imshow(sentiment_wordcloud, interpolation=\"bilinear\")\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(f\"Word Cloud for {sentiment.capitalize()} Sentiment\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've performed exploratory data analysis on the Austria Democracy Radar dataset. We've examined the structure of the data, analyzed sentiment distributions, and preprocessed the text for sentiment analysis.\n",
    "\n",
    "### Key Findings\n",
    "- The dataset contains text data with sentiment labels (positive, negative, neutral)\n",
    "- Text has been preprocessed and is ready for modeling\n",
    "- Word frequency analysis provides insights into the vocabulary used in governance texts\n",
    "\n",
    "### Next Steps\n",
    "1. **Data Preparation**: Finalize the preprocessing pipeline for the full dataset\n",
    "2. **Feature Engineering**: Create numerical features from the text data\n",
    "3. **Model Development**: Begin developing sentiment analysis models in the next notebook\n",
    "4. **Evaluation**: Set up metrics for evaluating model performance\n",
    "5. **Integration**: Prepare for integration with other tracks\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
